{"cells":[{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[],"source":["import gzip\n","import numpy as np\n","import os\n","import struct\n"]},{"cell_type":"markdown","metadata":{},"source":["# 1 获取数据\n","\n","一共4个数据包, 分别是 训练数据 及其标签, 验证数据 及其标签"]},{"cell_type":"code","execution_count":2,"metadata":{},"outputs":[],"source":["def load_train_images():\n","    with gzip.open('dataset/train-images-idx3-ubyte.gz', 'rb') as f:\n","        magic, n, rows, cols = struct.unpack('>IIII', f.read(16))\n","        assert magic == 2051\n","        return np.frombuffer(f.read(), dtype=np.uint8).reshape(n, rows, cols)\n","    \n","train_images_defalt = load_train_images()"]},{"cell_type":"code","execution_count":3,"metadata":{},"outputs":[],"source":["def load_train_labels():\n","    with gzip.open('dataset/train-labels-idx1-ubyte.gz', 'rb') as f:\n","        magic, n = struct.unpack('>II', f.read(8))\n","        assert magic == 2049\n","        return np.frombuffer(f.read(), dtype=np.uint8)\n","\n","train_labels_defalt = load_train_labels()"]},{"cell_type":"code","execution_count":4,"metadata":{},"outputs":[],"source":["def load_verify_images():\n","    with gzip.open('dataset/t10k-images-idx3-ubyte.gz', 'rb') as f:\n","        magic, n, rows, cols = struct.unpack('>IIII', f.read(16))\n","        assert magic == 2051\n","        return np.frombuffer(f.read(), dtype=np.uint8).reshape(n, rows, cols)\n","    \n","verify_images_defalt = load_verify_images()"]},{"cell_type":"code","execution_count":5,"metadata":{},"outputs":[],"source":["def load_verify_labels():\n","    with gzip.open('dataset/t10k-labels-idx1-ubyte.gz', 'rb') as f:\n","        magic, n = struct.unpack('>II', f.read(8))\n","        assert magic == 2049\n","        return np.frombuffer(f.read(), dtype=np.uint8)\n","\n","verify_labels_defalt = load_verify_labels()"]},{"cell_type":"markdown","metadata":{},"source":["## 1.1 数据预处理\n","\n","### 1.1.1 处理Labels: 把Labels转换为one-hot编码\n","防止网络认为数字有所含义, 影响网络判断"]},{"cell_type":"code","execution_count":6,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["5\n","[0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n"]}],"source":["train_labels_one_hot = np.zeros((train_labels_defalt.size, train_labels_defalt.max() + 1)) # 先全部初始化为0\n","train_labels_one_hot[np.arange(train_labels_defalt.size), train_labels_defalt] = 1 # 根据label的值，将对应的位置置为1\n","\n","verify_labels_one_hot = np.zeros((verify_labels_defalt.size, verify_labels_defalt.max() + 1)) \n","verify_labels_one_hot[np.arange(verify_labels_defalt.size), verify_labels_defalt] = 1 \n","\n","i = 0\n","print(train_labels_defalt[i])\n","print(train_labels_one_hot[i])\n","\n","\n","# 要用到的数据\n","train_labels = train_labels_one_hot\n","verify_labels = verify_labels_one_hot\n"]},{"cell_type":"markdown","metadata":{},"source":["### 1.1.2 处理Images: 把Images展平, 匹配输入 + 把像素灰度归一化\n","1. 展平: 二维图像 -> 一位数组 -> 输入层感知器\n","2. 归一化: 把[0, 255]的灰度值 映射到 [0, 1]区间内"]},{"cell_type":"code","execution_count":7,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["(60000, 784)\n","(784,)\n","[  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n","   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n","   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n","   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n","   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n","   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n","   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n","   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n","   0   0   0   0   0   0   0   0   3  18  18  18 126 136 175  26 166 255\n"," 247 127   0   0   0   0   0   0   0   0   0   0   0   0  30  36  94 154\n"," 170 253 253 253 253 253 225 172 253 242 195  64   0   0   0   0   0   0\n","   0   0   0   0   0  49 238 253 253 253 253 253 253 253 253 251  93  82\n","  82  56  39   0   0   0   0   0   0   0   0   0   0   0   0  18 219 253\n"," 253 253 253 253 198 182 247 241   0   0   0   0   0   0   0   0   0   0\n","   0   0   0   0   0   0   0   0  80 156 107 253 253 205  11   0  43 154\n","   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n","   0  14   1 154 253  90   0   0   0   0   0   0   0   0   0   0   0   0\n","   0   0   0   0   0   0   0   0   0   0   0   0   0 139 253 190   2   0\n","   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n","   0   0   0   0   0  11 190 253  70   0   0   0   0   0   0   0   0   0\n","   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0  35 241\n"," 225 160 108   1   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n","   0   0   0   0   0   0   0   0   0  81 240 253 253 119  25   0   0   0\n","   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n","   0   0  45 186 253 253 150  27   0   0   0   0   0   0   0   0   0   0\n","   0   0   0   0   0   0   0   0   0   0   0   0   0  16  93 252 253 187\n","   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n","   0   0   0   0   0   0   0 249 253 249  64   0   0   0   0   0   0   0\n","   0   0   0   0   0   0   0   0   0   0   0   0   0   0  46 130 183 253\n"," 253 207   2   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n","   0   0   0   0  39 148 229 253 253 253 250 182   0   0   0   0   0   0\n","   0   0   0   0   0   0   0   0   0   0   0   0  24 114 221 253 253 253\n"," 253 201  78   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n","   0   0  23  66 213 253 253 253 253 198  81   2   0   0   0   0   0   0\n","   0   0   0   0   0   0   0   0   0   0  18 171 219 253 253 253 253 195\n","  80   9   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n","  55 172 226 253 253 253 253 244 133  11   0   0   0   0   0   0   0   0\n","   0   0   0   0   0   0   0   0   0   0 136 253 253 253 212 135 132  16\n","   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n","   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n","   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n","   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n","   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n","   0   0   0   0   0   0   0   0   0   0]\n","[0.         0.         0.         0.         0.         0.\n"," 0.         0.         0.         0.         0.         0.\n"," 0.         0.         0.         0.         0.         0.\n"," 0.         0.         0.         0.         0.         0.\n"," 0.         0.         0.         0.         0.         0.\n"," 0.         0.         0.         0.         0.         0.\n"," 0.         0.         0.         0.         0.         0.\n"," 0.         0.         0.         0.         0.         0.\n"," 0.         0.         0.         0.         0.         0.\n"," 0.         0.         0.         0.         0.         0.\n"," 0.         0.         0.         0.         0.         0.\n"," 0.         0.         0.         0.         0.         0.\n"," 0.         0.         0.         0.         0.         0.\n"," 0.         0.         0.         0.         0.         0.\n"," 0.         0.         0.         0.         0.         0.\n"," 0.         0.         0.         0.         0.         0.\n"," 0.         0.         0.         0.         0.         0.\n"," 0.         0.         0.         0.         0.         0.\n"," 0.         0.         0.         0.         0.         0.\n"," 0.         0.         0.         0.         0.         0.\n"," 0.         0.         0.         0.         0.         0.\n"," 0.         0.         0.         0.         0.         0.\n"," 0.         0.         0.         0.         0.         0.\n"," 0.         0.         0.         0.         0.         0.\n"," 0.         0.         0.         0.         0.         0.\n"," 0.         0.         0.01176471 0.07058824 0.07058824 0.07058824\n"," 0.49411765 0.53333333 0.68627451 0.10196078 0.65098039 1.\n"," 0.96862745 0.49803922 0.         0.         0.         0.\n"," 0.         0.         0.         0.         0.         0.\n"," 0.         0.         0.11764706 0.14117647 0.36862745 0.60392157\n"," 0.66666667 0.99215686 0.99215686 0.99215686 0.99215686 0.99215686\n"," 0.88235294 0.6745098  0.99215686 0.94901961 0.76470588 0.25098039\n"," 0.         0.         0.         0.         0.         0.\n"," 0.         0.         0.         0.         0.         0.19215686\n"," 0.93333333 0.99215686 0.99215686 0.99215686 0.99215686 0.99215686\n"," 0.99215686 0.99215686 0.99215686 0.98431373 0.36470588 0.32156863\n"," 0.32156863 0.21960784 0.15294118 0.         0.         0.\n"," 0.         0.         0.         0.         0.         0.\n"," 0.         0.         0.         0.07058824 0.85882353 0.99215686\n"," 0.99215686 0.99215686 0.99215686 0.99215686 0.77647059 0.71372549\n"," 0.96862745 0.94509804 0.         0.         0.         0.\n"," 0.         0.         0.         0.         0.         0.\n"," 0.         0.         0.         0.         0.         0.\n"," 0.         0.         0.31372549 0.61176471 0.41960784 0.99215686\n"," 0.99215686 0.80392157 0.04313725 0.         0.16862745 0.60392157\n"," 0.         0.         0.         0.         0.         0.\n"," 0.         0.         0.         0.         0.         0.\n"," 0.         0.         0.         0.         0.         0.\n"," 0.         0.05490196 0.00392157 0.60392157 0.99215686 0.35294118\n"," 0.         0.         0.         0.         0.         0.\n"," 0.         0.         0.         0.         0.         0.\n"," 0.         0.         0.         0.         0.         0.\n"," 0.         0.         0.         0.         0.         0.\n"," 0.         0.54509804 0.99215686 0.74509804 0.00784314 0.\n"," 0.         0.         0.         0.         0.         0.\n"," 0.         0.         0.         0.         0.         0.\n"," 0.         0.         0.         0.         0.         0.\n"," 0.         0.         0.         0.         0.         0.04313725\n"," 0.74509804 0.99215686 0.2745098  0.         0.         0.\n"," 0.         0.         0.         0.         0.         0.\n"," 0.         0.         0.         0.         0.         0.\n"," 0.         0.         0.         0.         0.         0.\n"," 0.         0.         0.         0.         0.1372549  0.94509804\n"," 0.88235294 0.62745098 0.42352941 0.00392157 0.         0.\n"," 0.         0.         0.         0.         0.         0.\n"," 0.         0.         0.         0.         0.         0.\n"," 0.         0.         0.         0.         0.         0.\n"," 0.         0.         0.         0.31764706 0.94117647 0.99215686\n"," 0.99215686 0.46666667 0.09803922 0.         0.         0.\n"," 0.         0.         0.         0.         0.         0.\n"," 0.         0.         0.         0.         0.         0.\n"," 0.         0.         0.         0.         0.         0.\n"," 0.         0.         0.17647059 0.72941176 0.99215686 0.99215686\n"," 0.58823529 0.10588235 0.         0.         0.         0.\n"," 0.         0.         0.         0.         0.         0.\n"," 0.         0.         0.         0.         0.         0.\n"," 0.         0.         0.         0.         0.         0.\n"," 0.         0.0627451  0.36470588 0.98823529 0.99215686 0.73333333\n"," 0.         0.         0.         0.         0.         0.\n"," 0.         0.         0.         0.         0.         0.\n"," 0.         0.         0.         0.         0.         0.\n"," 0.         0.         0.         0.         0.         0.\n"," 0.         0.97647059 0.99215686 0.97647059 0.25098039 0.\n"," 0.         0.         0.         0.         0.         0.\n"," 0.         0.         0.         0.         0.         0.\n"," 0.         0.         0.         0.         0.         0.\n"," 0.         0.         0.18039216 0.50980392 0.71764706 0.99215686\n"," 0.99215686 0.81176471 0.00784314 0.         0.         0.\n"," 0.         0.         0.         0.         0.         0.\n"," 0.         0.         0.         0.         0.         0.\n"," 0.         0.         0.         0.         0.15294118 0.58039216\n"," 0.89803922 0.99215686 0.99215686 0.99215686 0.98039216 0.71372549\n"," 0.         0.         0.         0.         0.         0.\n"," 0.         0.         0.         0.         0.         0.\n"," 0.         0.         0.         0.         0.         0.\n"," 0.09411765 0.44705882 0.86666667 0.99215686 0.99215686 0.99215686\n"," 0.99215686 0.78823529 0.30588235 0.         0.         0.\n"," 0.         0.         0.         0.         0.         0.\n"," 0.         0.         0.         0.         0.         0.\n"," 0.         0.         0.09019608 0.25882353 0.83529412 0.99215686\n"," 0.99215686 0.99215686 0.99215686 0.77647059 0.31764706 0.00784314\n"," 0.         0.         0.         0.         0.         0.\n"," 0.         0.         0.         0.         0.         0.\n"," 0.         0.         0.         0.         0.07058824 0.67058824\n"," 0.85882353 0.99215686 0.99215686 0.99215686 0.99215686 0.76470588\n"," 0.31372549 0.03529412 0.         0.         0.         0.\n"," 0.         0.         0.         0.         0.         0.\n"," 0.         0.         0.         0.         0.         0.\n"," 0.21568627 0.6745098  0.88627451 0.99215686 0.99215686 0.99215686\n"," 0.99215686 0.95686275 0.52156863 0.04313725 0.         0.\n"," 0.         0.         0.         0.         0.         0.\n"," 0.         0.         0.         0.         0.         0.\n"," 0.         0.         0.         0.         0.53333333 0.99215686\n"," 0.99215686 0.99215686 0.83137255 0.52941176 0.51764706 0.0627451\n"," 0.         0.         0.         0.         0.         0.\n"," 0.         0.         0.         0.         0.         0.\n"," 0.         0.         0.         0.         0.         0.\n"," 0.         0.         0.         0.         0.         0.\n"," 0.         0.         0.         0.         0.         0.\n"," 0.         0.         0.         0.         0.         0.\n"," 0.         0.         0.         0.         0.         0.\n"," 0.         0.         0.         0.         0.         0.\n"," 0.         0.         0.         0.         0.         0.\n"," 0.         0.         0.         0.         0.         0.\n"," 0.         0.         0.         0.         0.         0.\n"," 0.         0.         0.         0.         0.         0.\n"," 0.         0.         0.         0.         0.         0.\n"," 0.         0.         0.         0.         0.         0.\n"," 0.         0.         0.         0.         0.         0.\n"," 0.         0.         0.         0.         0.         0.\n"," 0.         0.         0.         0.        ]\n"]}],"source":["train_images_flatten = train_images_defalt.reshape(train_images_defalt.shape[0], -1) # -1: 自动计算列数\n","verify_images_flatten = verify_images_defalt.reshape(verify_images_defalt.shape[0], -1)\n","\n","print(train_images_flatten.shape)\n","print(train_images_flatten[0].shape)\n","print(train_images_flatten[0])\n","\n","train_images_normalized = train_images_flatten / 255 # numpy会自动广播 (即对每个元素都除以255)\n","verify_images_normalized = verify_images_flatten / 255\n","\n","print(train_images_normalized[0])\n","\n","# 要用到的数据\n","train_images = train_images_normalized\n","verify_images = verify_images_normalized"]},{"cell_type":"markdown","metadata":{},"source":["# 2 定义工具函数\n"]},{"cell_type":"code","execution_count":14,"metadata":{},"outputs":[],"source":["# 前向传播所需的函数\n","\n","def sigmoid(x): # 输入层, 隐藏层的激活函数\n","    return 1 / (1 + np.exp(-x))\n","\n","def softmax(x): # 输出层的激活函数\n","    exp_x = np.exp(x - np.max(x, axis=1, keepdims=True))  # 减去最大值, 提高数值稳定性\n","    return exp_x / np.sum(exp_x, axis=1, keepdims=True)\n","\n","def cross_entropy_loss(y, y_hat): # 交叉熵损失函数\n","    return -np.sum(y * np.log(y_hat)) / y.shape[0]  # 平均损失\n","\n","# 反向传播所需的函数\n","\n","def sigmoid_derivative(x): # sigmoid的导数\n","    return x * (1 - x)\n","\n","def softmax_and_cross_entropy_derivative(y, y_hat): # softmax和交叉熵的导数\n","    return y_hat - y\n","\n","def matrix_derivative(x, delta):\n","    \"\"\"\n","    计算权重的梯度\n","    x: 输入矩阵 (batch_size, input_dim)\n","    delta: 误差矩阵 (batch_size, output_dim)\n","    return: 权重的梯度矩阵 (input_dim, output_dim)\n","    \"\"\"\n","    return np.dot(x.T, delta)\n","\n"]},{"cell_type":"markdown","metadata":{},"source":["# 3 定义网络"]},{"cell_type":"code","execution_count":20,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["[[1.27926930e-09 4.45472647e-05 2.62615408e-01 7.29289301e-01\n","  3.92676022e-03 3.17836729e-06 1.41676757e-06 1.80278292e-05\n","  3.89013178e-03 2.11227672e-04]]\n"]}],"source":["input_size = 28 * 28\n","hidden_size = 64\n","output_size = 10\n","\n","learning_rate = 0.01\n","\n","# 初始化\n","np.random.seed(0)\n","weights_input_hidden = np.random.randn(input_size, hidden_size) \n","weights_hidden_output = np.random.randn(hidden_size, output_size) \n","bias_input_hidden = np.zeros(hidden_size) \n","bias_hidden_output = np.zeros(output_size) \n","\n","\n","# 前向传播\n","def forward(x):\n","    global weights_input_hidden, weights_hidden_output, bias_input_hidden, bias_hidden_output # global: 用于在函数内部修改全局变量\n","    \n","    # 输入层到隐藏层\n","    hidden_layer_input = np.dot(x, weights_input_hidden) + bias_input_hidden # 线性方程\n","    hidden_layer_output = sigmoid(hidden_layer_input) # 激活函数\n","    \n","    # 隐藏层到输出层\n","    output_layer_input = np.dot(hidden_layer_output, weights_hidden_output) + bias_hidden_output # 线性方程\n","    output_layer_output = softmax(output_layer_input) # 激活函数\n","    \n","    # return output_output\n","    return hidden_layer_input, hidden_layer_output, output_layer_input, output_layer_output # 返回每一层的输入和输出, 以便反向传播\n","\n","# 测试前向传播\n","_, _, _, output = forward(train_images[0:1]) \n","print(output)     \n"]},{"cell_type":"markdown","metadata":{},"source":["# 4 Train\n","## 4.1 定义 反向传播 的细节"]},{"cell_type":"code","execution_count":21,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["9.146013013288108\n","8.045928443431055\n","6.858373726773531\n","5.645865574944836\n","4.483889711173232\n","3.312898899024671\n","2.015719967840419\n","0.7958909800969719\n","0.3193067000892973\n","0.20512802981463527\n"]}],"source":["def train(x, y):\n","    global weights_input_hidden, weights_hidden_output, bias_input_hidden, bias_hidden_output, learning_rate\n","    \n","    # 前向传播\n","    hidden_layer_input, hidden_layer_output, output_layer_input, output_layer_output = forward(x)\n","    \n","    # 反向传播\n","    loss = cross_entropy_loss(y, output_layer_output)\n","    \n","    # hidden to output layer\n","    ## 权重\n","    delta_hidden_output = softmax_and_cross_entropy_derivative(y, output_layer_output) # 误差: 交叉熵和softmax的导数\n","    weights_hidden_output_gradient = matrix_derivative(hidden_layer_output, delta_hidden_output) # 矩阵的导数 * 误差\n","    weights_hidden_output -= learning_rate * weights_hidden_output_gradient # 更新\n","    ## 偏置 (省略不讲)\n","    bias_hidden_output_gradient = softmax_and_cross_entropy_derivative(y, output_layer_output) \n","    bias_hidden_output -= learning_rate * bias_hidden_output_gradient.squeeze() # 更新\n","    \n","    # input to hidden layer (需要从最末尾开始计算)\n","    ## 权重\n","    delta_input_hidden = np.dot(delta_hidden_output, weights_hidden_output.T) * sigmoid_derivative(hidden_layer_output) # 误差: 上一层的误差 * 权重的转置 * sigmoid的导数\n","    weights_input_hidden_gradient = matrix_derivative(x, delta_input_hidden) # 矩阵的导数 * 误差(这里的误差是上一层的误差)\n","    weights_input_hidden -= learning_rate * weights_input_hidden_gradient # 更新\n","    ## 偏置 (省略不讲)\n","    bias_input_hidden_gradient = delta_input_hidden.sum(axis=0) # 求和\n","    bias_input_hidden -= learning_rate * bias_input_hidden_gradient # 更新\n","    \n","    return loss # 监视训练过程\n","\n","# 测试训练\n","index = 1\n","for _ in range(10):\n","    loss = train(train_images[index:index+1], train_labels[index:index+1]) # 因为使用了one-hot编码, 所以这里的x和y都是1x10的矩阵(二维), 而不是一个数\n","    print(loss)\n","\n"]},{"cell_type":"markdown","metadata":{},"source":["## 4.2 开始训练\n","\n","### 4.2.1 验证"]},{"cell_type":"code","execution_count":22,"metadata":{},"outputs":[],"source":["def verify(pred_label, verify_label):\n","    pred_label = np.argmax(pred_label, axis=1)\n","    verify_label = np.argmax(verify_label, axis=1)\n","    accuracy = np.sum(pred_label == verify_label) / verify_label.size\n","    return accuracy\n"]},{"cell_type":"code","execution_count":23,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["epoch: 0, index: 0, loss: 10.92628484431058\n","accuracy: 0.0856\n","epoch: 0, index: 10000, loss: 0.008538616562141365\n","accuracy: 0.7792\n","epoch: 0, index: 20000, loss: 0.3066869455090451\n","accuracy: 0.8303\n","epoch: 0, index: 30000, loss: 0.11922989485837657\n","accuracy: 0.8455\n","epoch: 0, index: 40000, loss: 0.031441357725519994\n","accuracy: 0.8656\n","epoch: 0, index: 50000, loss: 0.6625648020323737\n","accuracy: 0.8788\n","epoch: 1, index: 0, loss: 0.02501371106777871\n","accuracy: 0.8862\n","epoch: 1, index: 10000, loss: 0.2608398566220506\n","accuracy: 0.8809\n","epoch: 1, index: 20000, loss: 0.0447848574970419\n","accuracy: 0.8928\n","epoch: 1, index: 30000, loss: 3.6343061251751134\n","accuracy: 0.8983\n","epoch: 1, index: 40000, loss: 0.0013464339903792588\n","accuracy: 0.9018\n","epoch: 1, index: 50000, loss: 0.0017791042590510242\n","accuracy: 0.9049\n","epoch: 2, index: 0, loss: 0.0455747301817974\n","accuracy: 0.9069\n","epoch: 2, index: 10000, loss: 0.17703791054963994\n","accuracy: 0.9093\n","epoch: 2, index: 20000, loss: 0.00299624366758743\n","accuracy: 0.9104\n","epoch: 2, index: 30000, loss: 0.012758872475366721\n","accuracy: 0.9133\n","epoch: 2, index: 40000, loss: 0.04853558365657212\n","accuracy: 0.9143\n","epoch: 2, index: 50000, loss: 0.0026813579121877647\n","accuracy: 0.9149\n","epoch: 3, index: 0, loss: 1.0242234672122117\n","accuracy: 0.9167\n","epoch: 3, index: 10000, loss: 0.0030913235129503896\n","accuracy: 0.918\n","epoch: 3, index: 20000, loss: 0.0063987959030862105\n","accuracy: 0.9199\n","epoch: 3, index: 30000, loss: 0.5537665422491818\n","accuracy: 0.9213\n","epoch: 3, index: 40000, loss: 0.0007938607995908085\n","accuracy: 0.9241\n","epoch: 3, index: 50000, loss: 0.05144259852355367\n","accuracy: 0.9238\n","epoch: 4, index: 0, loss: 1.9701951251184273\n","accuracy: 0.9231\n","epoch: 4, index: 10000, loss: 0.006956703024573938\n","accuracy: 0.9232\n","epoch: 4, index: 20000, loss: 0.5865138101532713\n","accuracy: 0.9262\n","epoch: 4, index: 30000, loss: 0.19341610033046852\n","accuracy: 0.9256\n","epoch: 4, index: 40000, loss: 0.00043395738608521594\n","accuracy: 0.9298\n","epoch: 4, index: 50000, loss: 0.015414582300883332\n","accuracy: 0.9268\n"]}],"source":["# 开始训练\n","for epoch in range(5):\n","    # 打乱数据\n","    shuffle_index = np.random.permutation(60000)\n","    train_images = train_images[shuffle_index]\n","    train_labels = train_labels[shuffle_index]\n","    \n","    for index in range(60000):\n","        loss = train(train_images[index:index+1], train_labels[index:index+1])\n","        loss = loss.mean()\n","        if index % 10000 == 0:\n","            print(f'epoch: {epoch}, index: {index}, loss: {loss}')\n","            print(f'accuracy: {verify(forward(verify_images)[3], verify_labels)}')"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.11"}},"nbformat":4,"nbformat_minor":2}
